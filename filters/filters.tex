\documentclass[12pt,a4paper]{article}

% ====================
% Packages
% ====================
\usepackage[margin=1in]{geometry}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsbsy}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathbbol}
\usepackage{color}
\usepackage{xcolor}
\usepackage{float}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{array}
\usepackage{tikz}
\usepackage{listings}
\usepackage{tcolorbox}
\usepackage{enumitem}

% ====================
% Theorem Environments
% ====================
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{example}{Example}[section]

\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]

% ====================
% Code Listing Style
% ====================
\lstdefinestyle{pythonstyle}{
	language=Python,
	basicstyle=\ttfamily\small,
	keywordstyle=\color{blue}\bfseries,
	commentstyle=\color{gray}\itshape,
	stringstyle=\color{red},
	numbers=left,
	numberstyle=\tiny\color{gray},
	stepnumber=1,
	numbersep=8pt,
	backgroundcolor=\color{gray!5},
	frame=single,
	frameround=tttt,
	rulecolor=\color{black!30},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=4,
	showspaces=false,
	showstringspaces=false,
	captionpos=b,
	xleftmargin=15pt,
	xrightmargin=5pt
}
\lstset{style=pythonstyle}

% ====================
% Custom Commands
% ====================
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\argmax}{\operatorname*{arg\,max}}

% ====================
% Hyperref Setup
% ====================
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,      
	urlcolor=cyan,
	citecolor=blue,
	pdftitle={Particle Filter Guide},
	pdfpagemode=FullScreen,
}

% ====================
% Title Information
% ====================
\title{\textbf{Particle Filter and Differentiable Particle Filter}}
\author{}
\date{\today}

\begin{document}
	
	\maketitle
	
	\begin{abstract}
		This guide provides a practical, hands-on approach to understanding particle filters (PF) and differentiable particle filters (DPF). We follow the philosophy of ``getting your hands dirty first'' -- building intuition through implementation before diving into theoretical proofs. The guide covers the mathematical foundations, algorithmic details, and complete Python implementations of both standard and differentiable particle filters.
	\end{abstract}
	
	\tableofcontents
	\newpage
	
	% ====================
	% Introduction
	% ====================
	\section{Introduction}
	\label{sec:introduction}
	
	This guide provides a practical approach to understanding particle filters and their differentiable variants. The key insight is to start with implementation and visualization before exploring the underlying theory.
	
	\subsection{Learning Path}
	
	\begin{enumerate}[leftmargin=*]
		\item Start with basic particle filter implementation
		\item Understand the mathematical framework
		\item Transition to differentiable versions
		\item Apply to learning problems with gradient descent
	\end{enumerate}
	
	% ====================
	% Prerequisites
	% ====================
	\section{Prerequisites}
	\label{sec:prerequisites}
	
	\subsection{Required Knowledge}
	
	\begin{itemize}[leftmargin=*]
		\item Basic probability theory (Bayes' rule, conditional distributions)
		\item Python programming
		\item Basic linear algebra
		\item Familiarity with NumPy and PyTorch
	\end{itemize}
	
	\subsection{Software Requirements}
	
	The following Python packages are required:
	
	\begin{lstlisting}[language=Python]
		import numpy as np
		import matplotlib.pyplot as plt
		import torch
		import torch.nn as nn
	\end{lstlisting}
	
	% ====================
	% Mathematical Foundation
	% ====================
	\section{Mathematical Foundation}
	\label{sec:mathematical-foundation}
	
	\subsection{State Space Model (SSM)}
	
	The generative model assumes a Hidden Markov Model (HMM) structure with the following components:
	
	\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=State Space Model]
		\textbf{State Transition:}
		\begin{equation}
			x_t = f(x_{t-1}) + q_t, \quad q_t \sim \mathcal{N}(0, Q)
		\end{equation}
		
		\textbf{Observation:}
		\begin{equation}
			y_t = g(x_t) + r_t, \quad r_t \sim \mathcal{N}(0, R)
		\end{equation}
	\end{tcolorbox}
	
	\subsubsection{Toy Example Specification}
	
	In our illustrative example:
	\begin{itemize}
		\item \textbf{State transition:} $x_t = 0.9x_{t-1} + \epsilon_t$, where $\epsilon_t \sim \mathcal{N}(0, 0.5^2)$
		\item \textbf{Observation:} $y_t = x_t + \eta_t$, where $\eta_t \sim \mathcal{N}(0, 0.3^2)$
	\end{itemize}
	
	\textbf{Goal:} Estimate the posterior distribution $p(x_t \mid y_{1:t})$
	
	\subsection{Bayesian Recursion}
	
	The optimal Bayesian filtering solution follows a two-step recursion:
	
	\begin{tcolorbox}[colback=green!5!white,colframe=green!75!black,title=Bayesian Filter]
		\textbf{Prediction Step:}
		\begin{equation}
			p(x_t \mid y_{1:t-1}) = \int p(x_t \mid x_{t-1}) p(x_{t-1} \mid y_{1:t-1}) \, dx_{t-1}
		\end{equation}
		
		\textbf{Update Step:}
		\begin{equation}
			p(x_t \mid y_{1:t}) \propto p(y_t \mid x_t) p(x_t \mid y_{1:t-1})
		\end{equation}
	\end{tcolorbox}
	
	\begin{remark}
		This integral is intractable for nonlinear functions $f$ and $g$. The particle filter provides a Monte Carlo approximation to this recursion.
	\end{remark}
\section{The Kalman Filter: Optimality and Stability}
\label{sec:kalman-filter}

While Particle Filters (PF) provide a generalized solution for non-linear non-Gaussian systems [cite: 152], the Kalman Filter (KF) remains the optimal Minimum Mean Square Error (MMSE) estimator for Linear-Gaussian State Space Models (LGSSM)[cite: 196]. However, in practical financial engineering applications involving high-dimensional data or single-precision arithmetic (e.g., GPU computing), standard KF implementations often suffer from numerical instability. 

This section outlines the theoretical foundation of the LGSSM and derives the \textbf{Joseph stabilized form}, which guarantees the preservation of the covariance matrix's positive definiteness.

\subsection{Linear-Gaussian State Space Model}

We consider the classic Linear-Gaussian model as described in Example 2 of Doucet and Johansen (2011). The system is defined by the following stochastic difference equations:

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!50!black,title=LGSSM Definition]
	\textbf{State Evolution:}
	\begin{equation}
		x_t = F x_{t-1} + w_t, \quad w_t \sim \mathcal{N}(0, Q)
	\end{equation}
	\textbf{Observation:}
	\begin{equation}
		y_t = H x_t + v_t, \quad v_t \sim \mathcal{N}(0, R)
	\end{equation}
\end{tcolorbox}

where $x_t \in \mathbb{R}^{n_x}$ is the hidden state, $y_t \in \mathbb{R}^{n_y}$ is the observation, and $F, H, Q, R$ are matrices of appropriate dimensions[cite: 193, 194]. The noise terms $w_t$ and $v_t$ are assumed to be uncorrelated Gaussian white noise sequences.

\subsection{The Standard Kalman Recursion}

The recursive solution for the posterior density $p(x_t | y_{1:t}) = \mathcal{N}(x_t; \hat{x}_{t|t}, P_{t|t})$ is given by the standard Kalman Filter equations[cite: 248].

\textbf{1. Prediction (Time Update):}
\begin{align}
	\hat{x}_{t|t-1} &= F \hat{x}_{t-1|t-1} \\
	P_{t|t-1} &= F P_{t-1|t-1} F^T + Q
\end{align}

\textbf{2. Correction (Measurement Update):}
The optimal Kalman Gain $K_t$ minimizes the trace of the posterior covariance $P_{t|t}$:
\begin{equation}
	K_t = P_{t|t-1} H^T S_t^{-1}
\end{equation}
where $S_t = H P_{t|t-1} H^T + R$ is the innovation covariance. The state estimate is updated as:
\begin{equation}
	\hat{x}_{t|t} = \hat{x}_{t|t-1} + K_t (y_t - H \hat{x}_{t|t-1})
\end{equation}

\subsection{Numerical Instability and the Joseph Form}

\subsubsection{The Problem: Loss of Positive Definiteness}
The standard update equation for the error covariance matrix is derived as:
\begin{equation}
	\label{eq:standard_update}
	P_{t|t} = (I - K_t H) P_{t|t-1}
\end{equation}
Mathematically, Eq. \eqref{eq:standard_update} is correct. However, numerically, it is prone to instability. 
\begin{enumerate}
	\item \textbf{Asymmetry:} Due to floating-point round-off errors, the computation of $(I - K_t H) P_{t|t-1}$ may result in a non-symmetric matrix, violating the property that covariance matrices must be symmetric.
	\item \textbf{Indefiniteness:} Since Eq. \eqref{eq:standard_update} involves subtraction (implicitly within the $I - KH$ term), numerical errors can lead to $P_{t|t}$ having negative eigenvalues, rendering it non-positive definite. This causes the filter to diverge immediately.
\end{enumerate}

\subsubsection{The Solution: Joseph Stabilized Update}
To address this, we employ the \textbf{Joseph form} (also known as the symmetric update formula). It provides a numerically robust way to compute $P_{t|t}$:

\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Joseph Stabilized Update]
	\begin{equation}
		\label{eq:joseph_form}
		P_{t|t} = (I - K_t H) P_{t|t-1} (I - K_t H)^T + K_t R K_t^T
	\end{equation}
\end{tcolorbox}

\textbf{Theoretical Justification:}
Observe that Eq. \eqref{eq:joseph_form} is the sum of two terms:
\begin{itemize}
	\item The first term $(I - K_t H) P_{t|t-1} (I - K_t H)^T$ is a quadratic form. Since $P_{t|t-1}$ is positive definite (PD), this term is guaranteed to be positive semi-definite (PSD).
	\item The second term $K_t R K_t^T$ is also a quadratic form involving the covariance $R$, guaranteeing it is PSD.
\end{itemize}
The sum of two PSD matrices is always PSD. Therefore, the Joseph form structurally guarantees the symmetry and positive semi-definiteness of the updated covariance, making it robust against round-off errors.

\subsection{Stability Diagnostics: The Condition Number}

To monitor the numerical health of the filter during execution, we analyze the \textbf{Condition Number} $\kappa(P_t)$ of the covariance matrix.

\begin{definition}[Condition Number]
	For a symmetric positive definite covariance matrix $P$, the condition number is the ratio of its largest eigenvalue to its smallest eigenvalue:
	\begin{equation}
		\kappa(P) = \frac{|\lambda_{\max}(P)|}{|\lambda_{\min}(P)|}
	\end{equation}
\end{definition}

\textbf{Interpretation:}
\begin{itemize}
	\item \textbf{Well-conditioned ($\kappa \approx 1$):} The error distribution is spherical. Matrix inversion (required for $S_t^{-1}$) is numerically stable.
	\item \textbf{Ill-conditioned ($\kappa \gg 1$):} The uncertainty ellipsoid is extremely elongated (high certainty in some directions, high uncertainty in others). This leads to a loss of precision during the inversion of $S_t$, potentially causing the filter to "explode."
\end{itemize}

In our TensorFlow implementation (Section \ref{sec:kf-implementation}), we explicitly log $\kappa(P_t)$ at each step. A diverging condition number (e.g., $> 10^{15}$ for float64) serves as an early warning signal for numerical instability.


	\section{Standard Particle Filter}
	\label{sec:standard-pf}
	
	\subsection{Core Idea}
	
	Approximate the posterior distribution with weighted samples (particles):
	
	\begin{equation}
		p(x_t \mid y_{1:t}) \approx \sum_{i=1}^{N} w_t^{(i)} \delta(x_t - x_t^{(i)})
	\end{equation}
	
	where:
	\begin{itemize}
		\item $x_t^{(i)}$: particle $i$ at time $t$
		\item $w_t^{(i)}$: normalized weight of particle $i$
		\item $N$: total number of particles
		\item $\delta(\cdot)$: Dirac delta function
	\end{itemize}
	
	\subsection{Algorithm}
	
	\begin{tcolorbox}[colback=yellow!5!white,colframe=orange!75!black,title=Particle Filter Algorithm]
		
		\textbf{Initialization:} For $i = 1, \ldots, N$
		\begin{itemize}
			\item Sample $x_0^{(i)} \sim p(x_0)$
			\item Set $w_0^{(i)} = 1/N$
		\end{itemize}
		
		\textbf{For each time step} $t = 1, 2, \ldots, T$:
		
		\begin{enumerate}
			\item \textbf{Prediction:} For $i = 1, \ldots, N$
			\begin{equation}
				x_t^{(i)} \sim p(x_t \mid x_{t-1}^{(i)}) = \mathcal{N}(f(x_{t-1}^{(i)}), Q)
			\end{equation}
			
			\item \textbf{Weight Update:} For $i = 1, \ldots, N$
			\begin{equation}
				\tilde{w}_t^{(i)} = w_{t-1}^{(i)} \cdot p(y_t \mid x_t^{(i)})
			\end{equation}
			where the likelihood is:
			\begin{equation}
				p(y_t \mid x_t^{(i)}) = \frac{1}{\sqrt{2\pi R}} \exp\left(-\frac{(y_t - g(x_t^{(i)}))^2}{2R}\right)
			\end{equation}
			
			\item \textbf{Normalization:} For $i = 1, \ldots, N$
			\begin{equation}
				w_t^{(i)} = \frac{\tilde{w}_t^{(i)}}{\sum_{j=1}^{N} \tilde{w}_t^{(j)}}
			\end{equation}
			
			\item \textbf{State Estimation:}
			\begin{equation}
				\hat{x}_t = \sum_{i=1}^{N} w_t^{(i)} x_t^{(i)}
			\end{equation}
			
			\item \textbf{Resampling:} For $i = 1, \ldots, N$
			\begin{itemize}
				\item Draw $a_i \sim \text{Categorical}(w_t^{(1)}, \ldots, w_t^{(N)})$
				\item Set $x_t^{(i)} \leftarrow x_t^{(a_i)}$
				\item Reset $w_t^{(i)} \leftarrow 1/N$
			\end{itemize}
		\end{enumerate}
	\end{tcolorbox}
	
	\subsection{Why Resampling?}
	
	\begin{remark}
		Without resampling, most particles will have negligible weights after several iterations -- a phenomenon known as \textbf{particle degeneracy}. Resampling concentrates computational resources (particles) in high-probability regions of the state space.
	\end{remark}
	
	% ====================
	% Differentiable Particle Filter
	% ====================
	\section{Differentiable Particle Filter}
	\label{sec:differentiable-pf}
	
	\subsection{Motivation}
	
	\textbf{Problem with Standard PF:}
	\begin{itemize}
		\item The resampling step uses discrete sampling: $a_i \sim \text{Categorical}(w_t)$
		\item This operation is \textbf{non-differentiable}
		\item Cannot compute gradients $\frac{\partial \mathcal{L}}{\partial \theta}$
		\item Cannot use gradient descent to learn parameters $\theta = \{f_\theta, g_\theta, Q_\theta, R_\theta\}$
	\end{itemize}
	
	\textbf{Solution:} Replace discrete resampling with a \textbf{soft, differentiable approximation}.
	
	\subsection{Key Modification: Soft Resampling via Gumbel-Softmax}
	
	Replace categorical sampling with the \textbf{Gumbel-Softmax} trick:
	
	\begin{equation}
		\tilde{w}_t^{(i)} = \frac{\exp\left((\log w_t^{(i)} + g^{(i)}) / \tau\right)}{\sum_{j=1}^{N} \exp\left((\log w_t^{(j)} + g^{(j)}) / \tau\right)}
	\end{equation}
	
	where:
	\begin{itemize}
		\item $g^{(i)} \sim \text{Gumbel}(0,1)$ are i.i.d. Gumbel random variables
		\item $\tau > 0$ is the temperature parameter
		\item As $\tau \to 0$: recovers hard (categorical) resampling
		\item For $\tau > 0$: the operation becomes differentiable
	\end{itemize}
	
	\textbf{Gumbel Sampling in Code:}
	\begin{lstlisting}
		g = -torch.log(-torch.log(torch.rand_like(weights) + eps) + eps)
	\end{lstlisting}
	
	\subsection{Gradient Flow and End-to-End Learning}
	
	Since all operations are now differentiable, we can compute:
	\begin{equation}
		\frac{\partial \mathcal{L}}{\partial \theta}
	\end{equation}
	via backpropagation, where:
	\begin{itemize}
		\item $\theta$: learnable parameters in $f_\theta$, $g_\theta$, $Q_\theta$, $R_\theta$
		\item $\mathcal{L} = \sum_{t=1}^{T} \|y_t - \hat{y}_t\|_2^2$ or other task-specific loss
	\end{itemize}
	
	This enables \textbf{end-to-end learning} of latent dynamics and observation models.
	
	% ====================
	% Implementation
	% ====================

	
\section{Implementation under TensorFlow Framework}
\label{sec:kf-implementation}

We implement the filter using low-level tensor operations under the TensorFlow framework.

\subsection{The KalmanFilterTF Class}

\begin{lstlisting}[language=Python, caption=Multidimensional Kalman Filter in TensorFlow]
	import tensorflow as tf
	import numpy as np
	
	class KalmanFilterTF:
	"""
	TensorFlow implementation of Kalman Filter with Joseph Stabilized Update.
	"""
	def __init__(self, F, H, Q, R, x_init, P_init):
	# Ensure all inputs are float32 tensors
	self.F = tf.cast(F, dtype=tf.float32)
	self.H = tf.cast(H, dtype=tf.float32)
	self.Q = tf.cast(Q, dtype=tf.float32)
	self.R = tf.cast(R, dtype=tf.float32)
	self.x = tf.reshape(tf.cast(x_init, dtype=tf.float32), (-1, 1))
	self.P = tf.cast(P_init, dtype=tf.float32)
	
	def predict(self):
	"""Time Update: x = Fx, P = FPF' + Q"""
	self.x = tf.matmul(self.F, self.x)
	fp = tf.matmul(self.F, self.P)
	self.P = tf.matmul(fp, self.F, transpose_b=True) + self.Q
	return self.x, self.P
	
	def update(self, z_meas):
	"""
	Measurement Update using Joseph Form for Stability.
	"""
	z_meas = tf.reshape(tf.cast(z_meas, dtype=tf.float32), (-1, 1))
	
	# 1. Innovation
	z_pred = tf.matmul(self.H, self.x)
	y_residual = z_meas - z_pred
	
	# 2. Innovation Covariance S = HPH' + R
	hp = tf.matmul(self.H, self.P)
	S = tf.matmul(hp, self.H, transpose_b=True) + self.R
	
	# 3. Kalman Gain K = PH'S^{-1}
	# Using cholesky solve is preferred for stability if S is positive definite
	pht = tf.matmul(self.P, self.H, transpose_b=True)
	K = tf.matmul(pht, tf.linalg.inv(S)) 
	
	# 4. State Update
	self.x = self.x + tf.matmul(K, y_residual)
	
	# 5. Joseph Stabilized Covariance Update
	# P = (I-KH)P(I-KH)' + KRK'
	dim_x = tf.shape(self.P)[0]
	I = tf.eye(dim_x, dtype=tf.float32)
	I_KH = I - tf.matmul(K, self.H)
	
	p_term = tf.matmul(tf.matmul(I_KH, self.P), I_KH, transpose_b=True)
	r_term = tf.matmul(tf.matmul(K, self.R), K, transpose_b=True)
	self.P = p_term + r_term
	
	return self.x, self.P
\end{lstlisting}

\subsection{Standard Particle Filter in TensorFlow}

Here we implement the standard Particle Filter. Note that resampling (indexing) is non-differentiable, which motivates the Differentiable PF in the next section.

\begin{lstlisting}[language=Python, caption=Standard Particle Filter (TensorFlow)]
	def standard_particle_filter_tf(observations, n_particles=1000):
	T = len(observations)
	# Initialize particles (TensorFlow)
	particles = tf.random.normal((n_particles,), stddev=1.0)
	weights = tf.ones((n_particles,)) / n_particles
	
	estimates = []
	
	for t in range(T):
	# 1. Prediction (Transition)
	# x_t = 0.9 * x_{t-1} + noise
	noise = tf.random.normal((n_particles,), stddev=0.5)
	particles = 0.9 * particles + noise
	
	# 2. Weight Update (Likelihood)
	# y_t = x_t + noise
	obs = observations[t]
	likelihood = tf.exp(-0.5 * ((obs - particles) / 0.3)**2)
	weights *= likelihood
	weights /= tf.reduce_sum(weights) + 1e-9
	
	# 3. Estimation
	est = tf.reduce_sum(particles * weights)
	estimates.append(est)
	
	# 4. Multinomial Resampling (Non-differentiable)
	# We use tf.random.categorical for resampling indices
	logits = tf.math.log(weights + 1e-9)
	indices = tf.random.categorical(tf.reshape(logits, (1, -1)), n_particles)
	indices = tf.reshape(indices, (-1,))
	
	particles = tf.gather(particles, indices)
	weights = tf.ones((n_particles,)) / n_particles
	
	return tf.stack(estimates)
\end{lstlisting}

\subsection{Analysis: Numerical Stability via Condition Number}

A key metric for the stability of the Kalman Filter is the \textbf{Condition Number} of the covariance matrix $P$. It is defined as the ratio of the largest to smallest eigenvalue:
\begin{equation}
	\kappa(P) = \frac{|\lambda_{\max}(P)|}{|\lambda_{\min}(P)|}
\end{equation}

In our implementation, we monitor $\kappa(P)$ at each step. A diverging condition number (e.g., $> 10^{15}$ for float64) indicates that the matrix is becoming singular, which leads to severe numerical errors in calculating the Kalman Gain. The Joseph form update helps maintain a healthy condition number by preserving symmetry.

	\section{Advanced Topics}
	\label{sec:advanced}
	
	\subsection{Particle Degeneracy}
	
	\textbf{Problem:} After several iterations, most particle weights become negligible.
	
	\textbf{Solutions:}
	\begin{itemize}
		\item Standard resampling (for classical PF)
		\item Soft resampling (for DPF)
		\item Adaptive number of particles
		\item Regularization techniques
		\item Monitoring effective sample size: $\text{ESS} = 1 / \sum_{i=1}^{N} (w_t^{(i)})^2$
	\end{itemize}
	
	\subsection{Nonlinear Dynamics}
	
	Extend to nonlinear systems:
	
	\begin{lstlisting}
		# Example: Sine transition
		def nonlinear_transition(x, noise_std=0.5):
		return torch.sin(x) + torch.randn_like(x) * noise_std
		
		# Use in forward pass
		particles = nonlinear_transition(particles)
	\end{lstlisting}
	
	\subsection{Comparison with Other Methods}
	
	\begin{table}[H]
		\centering
		\caption{Comparison of Filtering Methods}
		\begin{tabular}{@{}lll@{}}
			\toprule
			\textbf{Method} & \textbf{Pros} & \textbf{Cons} \\
			\midrule
			Extended Kalman Filter & Fast, analytical & Fails for strong nonlinearity \\
			Particle Filter & Handles any nonlinearity & Not differentiable \\
			Differentiable PF & Learnable, handles nonlinearity & Computationally expensive \\
			\bottomrule
		\end{tabular}
	\end{table}
	
	% ====================
	% Summary
	% ====================
	\section{Summary}
	\label{sec:summary}
	
	\begin{table}[H]
		\centering
		\caption{Standard vs Differentiable Particle Filter}
		\begin{tabular}{@{}lll@{}}
			\toprule
			\textbf{Aspect} & \textbf{Standard PF} & \textbf{Differentiable PF} \\
			\midrule
			Transition & $x_t^{(i)} \sim p(x_t \mid x_{t-1}^{(i)})$ & Same \\
			Likelihood & $p(y_t \mid x_t^{(i)})$ & Same \\
			Weight Update & Hard normalization & Soft normalization \\
			Resampling & Categorical sampling & Gumbel-Softmax \\
			Gradient & Blocked & Flows via backprop \\
			Purpose & State estimation & Learnable inference \\
			\bottomrule
		\end{tabular}
	\end{table}
	
	% ====================
	% References
	% ====================

	% ====================
	% Appendix
	% ====================
	\appendix
	
	\section{Gumbel Distribution}
	\label{app:gumbel}
	
	The Gumbel distribution enables differentiable sampling from categorical distributions.
	
	\subsection{Gumbel-Max Trick}
	
	If $g_i \sim \text{Gumbel}(0,1)$ independently, then:
	\begin{equation}
		\argmax_i (\log \pi_i + g_i) \sim \text{Categorical}(\pi)
	\end{equation}
	
	\subsection{Gumbel-Softmax Relaxation}
	
	Replace $\argmax$ with $\text{softmax}$ for differentiability:
	\begin{equation}
		\tilde{\pi}_i = \frac{\exp((\log \pi_i + g_i)/\tau)}{\sum_j \exp((\log \pi_j + g_j)/\tau)}
	\end{equation}
	
	This provides a continuous, differentiable approximation to categorical sampling.

	
	
\end{document}